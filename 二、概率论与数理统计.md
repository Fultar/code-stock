# 一、概率论的基本概念
## 1.1 随机事件
随机现象：随机现象并不是真的无规律的随机，而是指结果的不确定性，其结果在大量的重复实验中具有统计规律性的现象。因此，概率论是研究和揭示随机现象统计规律性的一门数学学科。

随机试验：具备三要素：可以在相同的条件下重复进行；实验结果不唯一，但能事先明确所有可能的结果；在实验前不知道哪种结果会出现。

## 1.2 事件间的关系与运算：
$A\subset B$：B事件中包含A
$A=B$：事件A和事件B相等
$A \cup B$：和事件
$A \cap B$：积事件
$A-B$：差事件，A发生，B不发生
$A \cap B=\varnothing$：互斥事件
$A \cap B = \varnothing$且$A \cup B = S$：逆事件、对立事件


## 1.3 频率与频数
## 1.4 概率的性质：
1. 概率的非负性:$P(\varnothing) = 0$
2. 有限可加性：$P(A_1 \cup A_2 \cup ... \cup A_n) = P(A_1)+P(A_2)+...+P(A_n)$
3. $P(B-A) = P(B) - P(A); P(B)\neq P(A)$
4. 逆事件概率：$P(\bar A) = 1-P(A)$
5. 加法公式：$P(A \cup B) = P(A)+P(B)-P(AB)$

## 1.5 条件概率
1. **乘法公式**：$P(AB) = P(B|A)P(A)$
2. **全概率公式**：假设试验$E$的样本空间为$S$，$A$为$E$的事件，$B_1,B_2,\dots,B_n$为$S$的划分，且$P(B_i)>0 (i = 1,2,\cdots,n)$则
$$
P(A) = P(A|B_1)P(B_1)+P(A|B_2)P(B_2)+\cdots+P(A|B_n)P(B_n) 
$$
3. **贝叶斯公式**：假设试验$E$的样本空间为$S$，$A$为$E$的事件，$B_1,B_2,\dots,B_n$为$S$的划分，且$P(A)>0, P(B_i)>0 (i = 1,2,\cdots,n)$则
$$
P(B_i|A)=\frac{P(B_iA)}{P(A)} = \frac{P(A|B_i)P(B_i)}{\sum _{j=1}^{n}P(A|B_j)P(B_j)}
$$

### 1.6 独立性
1. 设$A、B$是两事件，且$P(A)>0$,$P(B|A)=P(B)$则$A,B$事件相互独立，反之亦然
2. 若$A,B$事件相互独立，则
$$
P(AB)=P(A)P(B)
$$

# 二、随机变量及其分布
## 2.1 离散型随机变量
### （一） 0-1分布
设随机变量$X$只可能取1和0两个值，$p$为发生概率，则它的分布律为：
$$
P\{X=k\} = p^k(1-p)^{1-k}, k = 0,1, (0<p<1)
$$

### （二）伯努利分布、二项分布
设试验$E$只有两个可能的结果：$A$ 和$\bar A$，则称$E$为伯努利实验（bernoulli distribution）。把$E$独立重复地进行$n$次，则称为$n$重伯努利试验。

如果某概率为$p$的事件在$n$次伯努利试验中发生$k$次，即在$(n-k)$次试验中不发生，记$q=1-p$，则其概率为：
$$P\{X=k\} = \left(
\begin{matrix}
n \\
k
\end{matrix}
\right) 
p^k(1-p)^{n-k}=
\left(
\begin{matrix}
n \\
k
\end{matrix}
\right) 
p^kq^{n-k} \\
\left(
\begin{matrix}
    n \\
    k
\end{matrix}
\right)=\frac{n!}{k!(n-k)!}=C_n^k
$$
$n$重伯努利分布也称二项分布(binomial distribution)，我们称随机变量$X$服从参数为$n,p$的二项分布，记作$X\sim b(n,p)$。

### （三）泊松分布
泊松分布其实是二项分布的一种极端情况，即试验次数$n \rightarrow \infin$，而事件的发生概率$p \rightarrow 0$，且通过过往的观察结果已知在$n$次试验中总是会发生$\lambda$次事件，记作$np_n=\lambda$（$\lambda$为常数）。这种情况称为泊松分布，可由二项分布推导而来：
$$
P(X=k)=\lim_{n\rightarrow \infin}\left(
\begin{matrix}
    n \\
    k
\end{matrix}
\right)p^k_n(1-p_n)^{n-k}=\frac{\lambda^ke^{-\lambda}}{k!}
$$

其中，$\lambda>0$是常数，称$X$服从参数为$\lambda$的泊松分布，记作$X \sim \pi(\lambda)$。

泊松分布因为其只需要知道$\lambda$（过往的观测结果），就能算出发生$k$次事件的概率。因此，泊松分布被广泛应用于计算小概率事件的发生概率，如计算空难的发生概率等。一般情况下，当$n\geq 20, \ p \leq 0.05$时即可用泊松分布。

## 2.2 连续型随机变量及其概率密度
### 2.2.1连续型随机变量的分布律
对于非离散型随机变量，我们并不能用固定值$k$来描述，而且我们对于这类连续型的变量更关心的是它落在在某一区间内的概率$P$。

设$X$是随机变量，$x$是任意实数，则分布函数为：
$$
F(x)=P\{X \leq x\}
$$
其中，$F(x)$为$X \leq x$的累积概率值。

对于任意实数$x_1, x_2(x_1 <x_2)$，有：
$$
P\{x_1<X \leq x_2\}=P\{X \leq x_2\}-P\{X \leq x_1\}=F(x_2)-F(x_1)
$$

### 2.2.2 连续型随机变量的概率密度
随机变量$X$的分布函数为$F(x)$，若存在非负函数$f(x)$，使任意实数$x$有
$$
F(x)=\int_{-\infin}^{x}f(t)dt,
$$
则称$X$为连续型随机变量，其中函数$f(x)$称为$X$的概率密度函数，简称概率密度。

对于任意实数$x_1, x_2(x_1 <x_2)$，有
$$
P\{x_1<X \leq x_2\}=F(x_2)-F(x_1)=\int_{x_1}^{x_2}f(x)dx
$$
#### （一）均匀分布（uniform distribution）
随机变量$X$具有概率密度：
$$
f(x)=\begin{cases}
    \frac{1}{b-a},\ a<x<b \\
    0
\end{cases}
$$
则称$X$在区间$(a,b)$内服从均匀分布，记作$X \sim U(a,b)$，其分布函数为：
$$
F(x) = \begin{cases}
    0,\ x<a \\
    \frac{x-a}{b-a}, \ a<x<b \\
    1, \ x>b
\end{cases}
$$

#### （二）指数分布
随机变量$X$具有概率密度：
$$
f(x)=\begin{cases}
    \frac{1}{\theta}e^{- \frac{x}{\theta}}, \ x>0 \\
    0, \ x<0
\end{cases}
$$

其中，$\theta>0$且为常数，则称$X$服从参数为$\theta$的指数分布，其分布函数为：
$$
F(x)= \begin{cases}
    1-e^{- \frac{x}{\theta}}, \ x>0 \\
    0,\ x<0
\end{cases}
$$

#### （三）正态分布
连续型随机变量$X$的概率密度为：
$$
f(x)=\frac{1}{\sqrt{2 \pi}\sigma}e^{- \frac{(x-\mu)^2}{2 \sigma^2}},\ -\infin<x<\infin
$$
其中，$\mu, \sigma \ (\sigma>0)$为常数，则称$X$服从参数为$\mu, \sigma$的正态分布，记作$X \sim N(\mu, \sigma^2)$。

其分布函数为：
$$
F(x) = \frac{1}{\sigma \sqrt{2 \pi}} \int_{- \infin}^{x}e^{- \frac{(t-\mu)^2}{2 \sigma ^2}}
$$


##### 正态分布的性质
1. 曲线关于$x=\mu$对称；$\mu$为位置参数，图像随$\mu$值的改变而在x轴上平移；$\sigma$与图像的峰值相关，$\sigma$越小，峰越高，$X$落在$\mu$附近的概率也越大。
2. 当$x=\mu$时有最大值：$$f(\mu)=\frac{1}{\sqrt{2 \pi}\sigma}$$

##### 标准正态分布
特别地，当$\mu = 0,\ \sigma = 1$时，$X$服从标准正态分布，其概率密度和分布函数分别用$\varphi(x), \ \Phi(x)$表示：
$$
\varphi(x) = \frac{1}{\sigma \sqrt{2 \pi}}e^{- \frac{t^2}{2}} \\
\Phi(x) = \frac{1}{\sqrt{2 \pi}} \int_{- \infin}^{x} e^{- \frac{t^2}{2}}dt
$$

类似地，我们把标准正态分布记作$X \sim N(0,1)$。

> 正态分布公式的推导较为复杂，是对三维体积切面求极限而来，只需记住这个公式即可。

# 三、二维随机变量
# 四、随机变量的数字特征
## 4.1 数学期望
设离散型随机变量$X$的分布律为：
$$
P\{X=x_k\}=p_k,\ k=0,1,2,\cdots
$$
则随机变量$X$的数学期望$E(X)$为：
$$
E(X)=\sum_{k=1}^{\infin}x_kp_k=\int_{-\infin}^{\infin}xf(x)dx
$$

数学期望简称期望，又称均值。
>期望是根据变量的分布律得到的，而平均值是由有限的试验统计得到的。如果试验次数趋近无穷，平均值便趋近期望。

### 常见分布的数学期望：
1. 泊松分布$X \sim \pi(\lambda)$：$E(X)=\lambda$
2. 均匀分布$X \sim U(a,b)$：$E(X)=\frac{a+b}{2}$
3. 0-1分布：$E(X)=p$
4. 二项分布：$E(X)=np$
5. 指数分布：$E(X)=\frac{1}{\theta}$
6. 正态分布$X \sim N(\mu,\sigma^2)$：$E(X)=\mu$
7. 标准正态分布$X \sim N(0,1)$：$E(x) = 0$

## 4.2 方差
方差指随机变量$X$与均值$E(X)$的偏离程度。显然，偏离程度可以用$E\{|X-E(X)|\}$表示，但绝对值运算不方便，因此，我们一般用$E\{[X-E(X)]^2\}$表示偏离程度，即方差，记作$D(X)$或$Var(X)$。

相应地，也存在标准差，又被称作均方差：$\sqrt{D(X)}=\sqrt{E\{[X-E(X)]^2\}}$，记作$\sigma(X)$。

### 方差的性质
1. 若$C$为常数，则$Var(X)=0.$
2. 随机变量$X$有$$Var(CX)=C^2 Var(X) \\ Var(X+C)=Var(X)$$
3. 若$X,Y$是两个随机变量，则
$$ 
Var(X+Y)=Var(X)+Var(Y)+2E\{[X-E(X)][Y-E(Y)]\}
$$
特别地，若$X,Y$相互独立，则
$$
Var(X+Y)=Var(X)+Var(Y)
$$

### 常见分布的方差：
1. 泊松分布$X \sim \pi(\lambda)$：$Var(X)=\lambda$
2. 0-1分布：$Var(X)=p(1-p)$
3. 二项分布：$Var(X)=np(1-p)$
4. 指数分布：$Var(X)=\frac{1}{\lambda^2}$
5. 正态分布$X \sim N(\mu,\sigma^2)$：$Var(X)=\sigma^2$
6. 均匀分布：$Var(X)=\frac{(b-a)^2}{12}$

### 切比雪夫不等式
设随机变量$X$具有数学期望$E(X)=\mu$，方差$Var(X)=\sigma^2$，则对于任意正整数$\epsilon$，有不等式
$$
P\{|X-\mu|\geq\epsilon\}\leq \frac{\sigma^2}{\epsilon^2}
$$

## 4.3 协方差
从上面的方差性质可知，当随机变量$X,Y$不相互独立时存在关系：
$$
Var(X+Y)=Var(X)+Var(Y)+2E\{[X-E(X)][Y-E(Y)]\}
$$
我们把$E\{(X-E(X))(Y-E(Y))\}$称作随机变量$X$与$Y$的协方差，记作$Cov(X,Y)$:
$$
Cov(X,Y)=E\{(X-E(X))(Y-E(Y))\}
$$
又把$\rho_{xy}$记作随机变量$X,Y$的相关系数：
$$
\rho_{xy}=\frac{Cov(X,Y)}{\sqrt{Var(X)} \sqrt{Var(Y)}}
$$
当$\rho_{xy}=0$时，随机变量$X$和$Y$不相关；而当$\rho_{xy}=1$时，随机变量$X$和$Y$线性相关。

### 协方差的性质
1. $Cov(X,Y)=E(X,Y)-E(X)E(Y).$
2. $Cov(X,X)=Var(X).$
3. $Coc(aX,bY)=abCov(X,Y).$
4. $Cov(X_1+X_2,Y)=Cov(X_1,Y)+Cov(X_2,Y).$